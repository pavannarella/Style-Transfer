{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "from torchvision import models,transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path,img_transform,size=(300,300)):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize(size,Image.LANCZOS)\n",
    "    image=img_transform(image).unsqueeze(0)\n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram(m):\n",
    "    # shape of m is (1,c,h,w)\n",
    "    _,c,h,w=m.size()\n",
    "    m=m.view(c , h*w)\n",
    "    m=torch.mm(m,m.t())\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_img(inp):\n",
    "    inp=inp.numpy().transpose((1,2,0)) # to convert chw to hwc\n",
    "    mean=np.array([0.485,0.456,0.406])\n",
    "    std=np.array([1,1,1])\n",
    "    inp=inp * std + mean\n",
    "    inp = np.clip(inp,0,1)\n",
    "    return inp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor,self).__init__()\n",
    "        self.selected_layers =[3,8,15,22]\n",
    "        self.vgg=models.vgg16(weights='VGG16_Weights.DEFAULT').features\n",
    "\n",
    "    def forward(self,x):\n",
    "        layer_feats=[]\n",
    "        for layer_num , layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if int(layer_num) in self.selected_layers:\n",
    "                layer_feats.append(x)\n",
    "        return layer_feats        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in models.vgg16(weights='VGG16_Weights.DEFAULT').features._modules.items():\n",
    "#     print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=(0.485,0.456,0.406),std=(1,1,1))])\n",
    "# 0.229,0.224,0.225\n",
    "content_img = get_image('content_image5.jpg',img_transform).to(device)\n",
    "style_img = get_image('style_image4.jpg',img_transform).to(device)\n",
    "\n",
    "generated_img=content_img.clone()\n",
    "generated_img.requires_grad = True\n",
    "optimizer = torch.optim.Adam([generated_img], lr = 0.0007,betas = (0.5,0.999))\n",
    "encoder=FeatureExtractor().to(device)\n",
    "\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0]\tContent Loss: 0.0000\tStyle Loss:78.9002\n",
      "Epoch [100]\tContent Loss: 0.9436\tStyle Loss:25.6521\n",
      "Epoch [200]\tContent Loss: 1.2161\tStyle Loss:14.9854\n",
      "Epoch [300]\tContent Loss: 1.3519\tStyle Loss:10.2414\n",
      "Epoch [400]\tContent Loss: 1.4237\tStyle Loss:7.7343\n",
      "Epoch [500]\tContent Loss: 1.4693\tStyle Loss:6.1720\n",
      "Epoch [600]\tContent Loss: 1.5041\tStyle Loss:5.0809\n",
      "Epoch [700]\tContent Loss: 1.5302\tStyle Loss:4.2682\n",
      "Epoch [800]\tContent Loss: 1.5508\tStyle Loss:3.6306\n",
      "Epoch [900]\tContent Loss: 1.5674\tStyle Loss:3.1170\n"
     ]
    }
   ],
   "source": [
    "# generated_img=generated_img.to(device)\n",
    "content_weight=1\n",
    "style_weight=100\n",
    "# f=1\n",
    "for epoch in range(1000):\n",
    "\n",
    "    content_features = encoder(content_img)\n",
    "    style_features = encoder(style_img)\n",
    "    generated_features = encoder(generated_img)\n",
    "\n",
    "    content_loss = torch.mean((content_features[-1] - generated_features[-1])**2)\n",
    "\n",
    "    style_loss = 0\n",
    "    for gf , sf in zip(generated_features , style_features):\n",
    "        _,c,h,w=gf.size()\n",
    "        gram_gf=get_gram(gf)\n",
    "        gram_sf=get_gram(sf)\n",
    "        style_loss += torch.mean((gram_gf-gram_sf)**2)/(c * h * w)\n",
    "    \n",
    "    loss = content_weight * content_loss + style_weight * style_loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100 ==0:\n",
    "        print(\"Epoch [{}]\\tContent Loss: {:.4f}\\tStyle Loss:{:.4f}\".format(epoch,content_loss.item(),style_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = generated_img.detach().cpu().squeeze()\n",
    "inp = denormalize_img(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show(inp.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow('aa',inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "def showimg(tensor):\n",
    "    transform = T.ToPILImage()\n",
    "    img = transform(tensor)\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = inp*255.0\n",
    "tensor = np.array(tensor, dtype=np.uint8)\n",
    "if np.ndim(tensor)>3:\n",
    "  assert tensor.shape[0] == 1\n",
    "  tensor = tensor[0]\n",
    "showimg(tensor)\n",
    "# tensor =  PIL.Image.fromarray(tensor)\n",
    "# plt.imshow(cv2.cvtColor(np.array(tensor), cv2.COLOR_BGR2RGB))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d76c810ee3d1f8a94fe1df69ec88d551d75970a15d65ed56765eda9e40c1b9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
